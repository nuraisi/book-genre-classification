{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YILp58KeQBES"
   },
   "source": [
    "**INSTALL MODUL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdCQSx3bMbRo",
    "outputId": "9c0ceb21-2076-4cb6-f433-52a039217da6"
   },
   "outputs": [],
   "source": [
    "%pip install regex\n",
    "%pip install scikeras\n",
    "%pip install langdetect\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlxtend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJNGigfyQH9s"
   },
   "source": [
    "**IMPORT MODUL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekvt7uupMhYZ",
    "outputId": "1526e447-d0dd-4515-9e72-65c369f7becb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nest_asyncio\n",
    "import datetime as dt\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nest_asyncio\n",
    "import datetime as dt\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMnrisGVQL5u"
   },
   "source": [
    "**SCRAPPING WEB GOODREADS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NG97yl709ch3",
    "outputId": "fc5f35f6-39f4-41cf-8f90-70494f451e6b"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "urls = [\n",
    "    \"https://www.goodreads.com/book/show/2452401.Teatro_Grottesco\",\n",
    "    \"https://www.goodreads.com/book/show/42372424-the-laws-of-the-skies\",\n",
    "    \"https://www.goodreads.com/book/show/20893407-the-lesser-dead\",\n",
    "    \"https://www.goodreads.com/book/show/35356382-the-chalk-man\",\n",
    "    \"https://www.goodreads.com/book/show/51160099-thirteen-storeys\",\n",
    "    \"https://www.goodreads.com/book/show/12341557-the-wind-through-the-keyhole\",\n",
    "    \"https://www.goodreads.com/book/show/472966.The_Vampyre\",\n",
    "    \"https://www.goodreads.com/book/show/6572.Suffer_the_Children\",\n",
    "    \"https://www.goodreads.com/book/show/11806716-blackout\",\n",
    "    \"https://www.goodreads.com/book/show/18667769-the-merciless\",\n",
    "    \"https://www.goodreads.com/book/show/31341.Blood_And_Gold\",\n",
    "    \"https://www.goodreads.com/book/show/30065.The_Walking_Dead_Vol_5\",\n",
    "    \"https://www.goodreads.com/book/show/16057298-north-american-lake-monsters\",\n",
    "    \"https://www.goodreads.com/book/show/45890523-starve-acre\",\n",
    "    \"https://www.goodreads.com/book/show/53329253-echo\",\n",
    "    \"https://www.goodreads.com/book/show/31343.Pandora\",\n",
    "    \"https://www.goodreads.com/book/show/63213.The_Dunwich_Horror_and_Others\",\n",
    "    \"https://www.goodreads.com/book/show/3978.A_Winter_Haunting\",\n",
    "    \"https://www.goodreads.com/book/show/8779896-the-monkey-s-paw\",\n",
    "    \"https://www.goodreads.com/book/show/63264519-dead-of-winter\",\n",
    "    \"https://www.goodreads.com/book/show/239065.The_Descent\",\n",
    "    \"https://www.goodreads.com/book/show/291950.Books_of_Blood_Volume_Two\",\n",
    "    \"https://www.goodreads.com/book/show/10577.Roadwork\",\n",
    "    \"https://www.goodreads.com/book/show/62919399-nineteen-claws-and-a-black-bird\",\n",
    "    \"https://www.goodreads.com/book/show/891472.The_Summer_I_Died\",\n",
    "    \"https://www.goodreads.com/book/show/10577.Roadwork\",\n",
    "    \"https://www.goodreads.com/book/show/202795132-la-riada\",\n",
    "    \"https://www.goodreads.com/book/show/219362.Legion\",\n",
    "    \"https://www.goodreads.com/book/show/145152.The_Cellar\",\n",
    "    \"https://www.goodreads.com/book/show/58371432-the-book-of-cold-cases\",\n",
    "    \"https://www.goodreads.com/book/show/6465707-the-walking-dead\",\n",
    "    \"https://www.goodreads.com/book/show/8051458-the-reapers-are-the-angels\",\n",
    "    \"https://www.goodreads.com/book/show/49081449-venus-in-the-blind-spot\",\n",
    "    \"https://www.goodreads.com/book/show/911954.The_Imago_Sequence_and_Other_Stories\",\n",
    "    \"https://www.goodreads.com/book/show/59093587-patricia-wants-to-cuddle\",\n",
    "    \"https://www.goodreads.com/book/show/38922230-ghost-wall\",\n",
    "    \"https://www.goodreads.com/book/show/693172.Twilight_Eyes\",\n",
    "    \"https://www.goodreads.com/book/show/24040551-the-night-sister\",\n",
    "    \"https://www.goodreads.com/book/show/31776292-frankenstein\",\n",
    "    \"https://www.goodreads.com/book/show/32828538-lost-boy\",\n",
    "    \"https://www.goodreads.com/book/show/23308488-chilling-adventures-of-sabrina-vol-1\",\n",
    "    \"https://www.goodreads.com/book/show/308540.The_Haunted_Mask\",\n",
    "    \"https://www.goodreads.com/book/show/1137702.The_Wendigo\",\n",
    "    \"https://www.goodreads.com/book/show/39325105-harrow-the-ninth\",\n",
    "    \"https://www.goodreads.com/book/show/125542.Stay_Out_of_the_Basement\",\n",
    "    \"https://www.goodreads.com/book/show/10618.Apt_Pupil\",\n",
    "    \"https://www.goodreads.com/book/show/64948.Whispers\",\n",
    "    \"https://www.goodreads.com/book/show/13030260-the-devil-in-silver\",\n",
    "    \"https://www.goodreads.com/book/show/33574090-what-the-hell-did-i-just-read\",\n",
    "    \"https://www.goodreads.com/book/show/46041168-hurricane-season\",\n",
    "    \"https://www.goodreads.com/book/show/38458438-the-houseguest-and-other-stories\",\n",
    "    \"https://www.goodreads.com/book/show/13542949-your-house-is-on-fire-your-children-all-gone\",\n",
    "    \"https://www.goodreads.com/book/show/10129880-the-colour-out-of-space\",\n",
    "    \"https://www.goodreads.com/book/show/45685.The_Canterville_Ghost\",\n",
    "    \"https://www.goodreads.com/book/show/60708721-it-came-from-the-closet\",\n",
    "    \"https://www.goodreads.com/book/show/10511555-dracula-s-guest\",\n",
    "    \"https://www.goodreads.com/book/show/21570066-bones-all\",\n",
    "    \"https://www.goodreads.com/book/show/8957.Tunnels_of_Blood\",\n",
    "    \"https://www.goodreads.com/book/show/196848553-the-house-that-horror-built\",\n",
    "    \"https://www.goodreads.com/book/show/58502336-the-nice-house-on-the-lake-vol-1\",\n",
    "    \"https://www.goodreads.com/book/show/51610977-voices-in-the-snow\",\n",
    "    \"https://www.goodreads.com/book/show/43317482-in-the-dream-house\",\n",
    "    \"https://www.goodreads.com/book/show/28233096-the-silver-eyes\",\n",
    "    \"https://www.goodreads.com/book/show/6420846-lockdown\",\n",
    "    \"https://www.goodreads.com/book/show/228204.Demon_Seed\",\n",
    "    \"https://www.goodreads.com/book/show/204294824-sleep-tight\",\n",
    "    \"https://www.goodreads.com/book/show/18782854-the-supernatural-enhancements\",\n",
    "    \"https://www.goodreads.com/book/show/207611566-witchcraft-for-wayward-girls\",\n",
    "    \"https://www.goodreads.com/book/show/55711279-something-is-killing-the-children-vol-3\",\n",
    "    \"https://www.goodreads.com/book/show/16299.And_Then_There_Were_None\",\n",
    "    \"https://www.goodreads.com/book/show/18453110-the-three\",\n",
    "    \"https://www.goodreads.com/book/show/6739080-hater\",\n",
    "    \"https://www.goodreads.com/book/show/397861.The_Dark\",\n",
    "    \"https://www.goodreads.com/book/show/57147078-deserter\",\n",
    "    \"https://www.goodreads.com/book/show/11550.Usher_s_Passing\",\n",
    "    \"https://www.goodreads.com/book/show/397872.Lair\",\n",
    "    \"https://www.goodreads.com/book/show/477801.Let_s_Go_Play_at_the_Adams_\",\n",
    "    \"https://www.goodreads.com/book/show/14059024-the-whispering-skull\",\n",
    "    \"https://www.goodreads.com/book/show/25365.Out\",\n",
    "    \"https://www.goodreads.com/book/show/49183687-harrow-lake\",\n",
    "    \"https://www.goodreads.com/book/show/18748653-daughters-unto-devils\",\n",
    "    \"https://www.goodreads.com/book/show/53869658-secret-santa\",\n",
    "    \"https://www.goodreads.com/book/show/52102795-dear-laura\",\n",
    "    \"https://www.goodreads.com/book/show/49770175-camp-slaughter\",\n",
    "    \"https://www.goodreads.com/book/show/58763686-haunting-adeline\",\n",
    "    \"https://www.goodreads.com/book/show/28870482-the-haunting-of-blackwood-house\",\n",
    "    \"https://www.goodreads.com/book/show/61317867-tombs\",\n",
    "    \"https://www.goodreads.com/book/show/20344877-we-are-all-completely-fine\",\n",
    "    \"https://www.goodreads.com/book/show/10822174-the-walking-dead-vol-14\",\n",
    "    \"https://www.goodreads.com/book/show/40495148-blindness\",\n",
    "    \"https://www.goodreads.com/book/show/32703.The_Diary_of_Ellen_Rimbauer\",\n",
    "    \"https://www.goodreads.com/book/show/201646.The_Woods_Are_Dark\",\n",
    "    \"https://www.goodreads.com/book/show/16435.Life_Expectancy\",\n",
    "    \"https://www.goodreads.com/book/show/7507908-the-replacement\",\n",
    "    \"https://www.goodreads.com/book/show/195945770-in-bloom\",\n",
    "    \"https://www.goodreads.com/book/show/40091283-the-hiding-place\",\n",
    "    \"https://www.goodreads.com/book/show/15799400-the-resurrectionist\"\n",
    "\n",
    "]\n",
    "\n",
    "with open('goodreads_books4.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Author\", \"Link\", \"Sinopsis\"])\n",
    "\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        title_tag = soup.find('h1', class_='Text Text__title1', attrs={'data-testid': 'bookTitle'})\n",
    "        title = title_tag.text.strip() if title_tag else \"Title not found\"\n",
    "\n",
    "        author_tag = soup.find('span', class_='ContributorLink__name', attrs={'data-testid': 'name'})\n",
    "        author = author_tag.text.strip() if author_tag else \"Author not found\"\n",
    "\n",
    "        sinopsis_tag = soup.find('span', class_='Formatted')\n",
    "        sinopsis = sinopsis_tag.text.strip() if sinopsis_tag else \"Sinopsis not found\"\n",
    "\n",
    "        writer.writerow([title, author, url, sinopsis])\n",
    "        print(f\"Title: {title}, Author: {author}, Link: {url}, Sinopsis: {sinopsis}\")\n",
    "\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKwEP8TmQVzA"
   },
   "source": [
    "**DATA CLEANING DATASET GENRE ROMANTIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5b-gnwBjNBN",
    "outputId": "01d880a3-0b58-4747-8b18-2ff35f964f62"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def bersihkan_teks(teks):\n",
    "    teks_bersih = re.sub(r'[^a-zA-Z0-9\\s.,!?()-]', '', str(teks))\n",
    "    teks_bersih = ' '.join(teks_bersih.split())\n",
    "    return teks_bersih\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        \"/content/romance_clean.csv\",\n",
    "        sep=';',\n",
    "        quotechar='\"',\n",
    "        escapechar='\\\\',\n",
    "        on_bad_lines='skip',\n",
    "        engine='python'\n",
    "    )\n",
    "\n",
    "    print(\"Nama kolom dalam dataset:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    if len(df.columns) == 1:\n",
    "        df = pd.read_csv(\n",
    "            \"/content/romance_clean.csv\",\n",
    "            sep=',',\n",
    "            quotechar='\"',\n",
    "            escapechar='\\\\',\n",
    "            on_bad_lines='skip',\n",
    "            engine='python',\n",
    "            header=None\n",
    "        )\n",
    "        df.columns = ['Title', 'Author', 'Link', 'Sinopsis', 'Genre']\n",
    "\n",
    "    df_awal = df.copy()\n",
    "    jumlah_awal = len(df)\n",
    "    print(f\"\\nJumlah baris awal: {jumlah_awal}\")\n",
    "\n",
    "    df = df[df['Sinopsis'].notna()]\n",
    "    df = df[df['Sinopsis'].str.strip() != '']\n",
    "    df = df[df['Genre'] != 'Genre not found']W\n",
    "    df['Sinopsis'] = df['Sinopsis'].apply(bersihkan_teks)\n",
    "    df = df.drop_duplicates(subset='Sinopsis', keep='first')\n",
    "    data_dihapus = df_awal[~df_awal.index.isin(df.index)]\n",
    "    jumlah_akhir = len(df)\n",
    "    jumlah_dihapus = jumlah_awal - jumlah_akhir\n",
    "\n",
    "    print(f\"\\nJumlah baris yang dihapus: {jumlah_dihapus}\")\n",
    "    print(f\"Jumlah baris yang tersisa: {jumlah_akhir}\")\n",
    "    print(\"\\nSampel data yang dihapus:\")\n",
    "    print(data_dihapus.head())\n",
    "    df.to_csv(\"romancee_clean.csv\", index=False, sep=',')\n",
    "    print(\"\\nData yang telah dibersihkan disimpan ke 'romancee_clean.csv'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkmxptyYQk1H"
   },
   "source": [
    "**DATA CLEANING DATASET GENRE FIKSI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_sPg4PeEk-Wf",
    "outputId": "2f805ea1-ac98-40bd-c5a7-a3f125b9a9a4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def bersihkan_teks(teks):\n",
    "    teks_bersih = re.sub(r'[^a-zA-Z0-9\\s.,!?()-]', '', str(teks))\n",
    "    teks_bersih = ' '.join(teks_bersih.split())\n",
    "    return teks_bersih\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        \"/content/fiction_clean.csv\",\n",
    "        sep=';',\n",
    "        quotechar='\"',\n",
    "        escapechar='\\\\',\n",
    "        on_bad_lines='skip',\n",
    "        engine='python'\n",
    "    )\n",
    "\n",
    "    print(\"Nama kolom dalam dataset:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    if len(df.columns) == 1:\n",
    "        df = pd.read_csv(\n",
    "            \"/content/fiction_clean.csv\",\n",
    "            sep=',',\n",
    "            quotechar='\"',\n",
    "            escapechar='\\\\',\n",
    "            on_bad_lines='skip',\n",
    "            engine='python',\n",
    "            header=None\n",
    "        )\n",
    "        df.columns = ['Title', 'Author', 'Link', 'Sinopsis', 'Genre']\n",
    "\n",
    "    df_awal = df.copy()\n",
    "    jumlah_awal = len(df)\n",
    "    print(f\"\\nJumlah baris awal: {jumlah_awal}\")\n",
    "\n",
    "    df = df[df['Sinopsis'].notna()]\n",
    "    df = df[df['Sinopsis'].str.strip() != '']\n",
    "    df = df[df['Genre'] != 'Genre not found']\n",
    "    df['Sinopsis'] = df['Sinopsis'].apply(bersihkan_teks)\n",
    "    df = df.drop_duplicates(subset='Sinopsis', keep='first')\n",
    "    data_dihapus = df_awal[~df_awal.index.isin(df.index)]\n",
    "    jumlah_akhir = len(df)\n",
    "    jumlah_dihapus = jumlah_awal - jumlah_akhir\n",
    "\n",
    "    print(f\"\\nJumlah baris yang dihapus: {jumlah_dihapus}\")\n",
    "    print(f\"Jumlah baris yang tersisa: {jumlah_akhir}\")\n",
    "    print(\"\\nSampel data yang dihapus:\")\n",
    "    print(data_dihapus.head())\n",
    "    df.to_csv(\"fictionn_clean.csv\", index=False, sep=',')\n",
    "    print(\"\\nData yang telah dibersihkan disimpan ke 'fictionn_clean.csv'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qlylk82AQnkv"
   },
   "source": [
    "**DATA CLEANING DATASET GENRE POLITIK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a4vObhjlocK",
    "outputId": "5d712519-460a-4d1e-9b5a-a635fc290074"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def bersihkan_teks(teks):\n",
    "    teks_bersih = re.sub(r'[^a-zA-Z0-9\\s.,!?()-]', '', str(teks))\n",
    "    teks_bersih = ' '.join(teks_bersih.split())\n",
    "    return teks_bersih\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        \"/content/political_clean.csv\",\n",
    "        sep=';',\n",
    "        quotechar='\"',\n",
    "        escapechar='\\\\',\n",
    "        on_bad_lines='skip',\n",
    "        engine='python'\n",
    "    )\n",
    "\n",
    "    if len(df.columns) == 1:\n",
    "        df = pd.read_csv(\n",
    "            \"/content/political_clean.csv\",\n",
    "            sep=',',\n",
    "            quotechar='\"',\n",
    "            escapechar='\\\\',\n",
    "            on_bad_lines='skip',\n",
    "            engine='python',\n",
    "            header=None\n",
    "        )\n",
    "        df.columns = ['Title', 'Author', 'Link', 'Sinopsis', 'Genre']\n",
    "\n",
    "    df_awal = df.copy()\n",
    "\n",
    "    jumlah_awal = len(df)\n",
    "\n",
    "    df = df[df['Sinopsis'].notna()]\n",
    "    df = df[df['Sinopsis'].str.strip() != '']\n",
    "    df = df[df['Genre'] != 'Genre not found']\n",
    "    df['Sinopsis'] = df['Sinopsis'].apply(bersihkan_teks)\n",
    "    df = df.drop_duplicates(subset='Sinopsis', keep='first')\n",
    "\n",
    "    data_dihapus = df_awal[~df_awal.index.isin(df.index)]\n",
    "\n",
    "    jumlah_akhir = len(df)\n",
    "    jumlah_dihapus = jumlah_awal - jumlah_akhir\n",
    "\n",
    "    print(f\"Jumlah baris yang dihapus: {jumlah_dihapus}\")\n",
    "    print(f\"Jumlah baris yang tersisa: {jumlah_akhir}\")\n",
    "    print(\"Sampel data yang dihapus:\")\n",
    "    print(data_dihapus.head())\n",
    "\n",
    "    df.to_csv(\"politicall_clean.csv\", index=False, sep=',')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8cDatvYQsbc"
   },
   "source": [
    "**DATA CLEANING DATASET GENRE NON FIKSI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0mQERsVml8rS",
    "outputId": "3bc0a99d-c7ed-468a-caaa-0d6a8a3aebf0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def bersihkan_teks(teks):\n",
    "    teks_bersih = re.sub(r'[^a-zA-Z0-9\\s.,!?()-]', '', str(teks))\n",
    "    teks_bersih = ' '.join(teks_bersih.split())\n",
    "    return teks_bersih\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        \"/content/nonfiction_clean.csv\",\n",
    "        sep=';',\n",
    "        quotechar='\"',\n",
    "        escapechar='\\\\',\n",
    "        on_bad_lines='skip',\n",
    "        engine='python'\n",
    "    )\n",
    "\n",
    "    if len(df.columns) == 1:\n",
    "        df = pd.read_csv(\n",
    "            \"/content/nonfiction_clean.csv\",\n",
    "            sep=',',\n",
    "            quotechar='\"',\n",
    "            escapechar='\\\\',\n",
    "            on_bad_lines='skip',\n",
    "            engine='python',\n",
    "            header=None\n",
    "        )\n",
    "        df.columns = ['Title', 'Author', 'Link', 'Sinopsis', 'Genre']\n",
    "\n",
    "    df_awal = df.copy()\n",
    "\n",
    "    jumlah_awal = len(df)\n",
    "\n",
    "    df = df[df['Sinopsis'].notna()]\n",
    "    df = df[df['Sinopsis'].str.strip() != '']\n",
    "    df = df[df['Genre'] != 'Genre not found']\n",
    "    df['Sinopsis'] = df['Sinopsis'].apply(bersihkan_teks)\n",
    "    df = df.drop_duplicates(subset='Sinopsis', keep='first')\n",
    "\n",
    "    data_dihapus = df_awal[~df_awal.index.isin(df.index)]\n",
    "\n",
    "    jumlah_akhir = len(df)\n",
    "    jumlah_dihapus = jumlah_awal - jumlah_akhir\n",
    "\n",
    "    print(f\"Jumlah baris yang dihapus: {jumlah_dihapus}\")\n",
    "    print(f\"Jumlah baris yang tersisa: {jumlah_akhir}\")\n",
    "    print(\"Sampel data yang dihapus:\")\n",
    "    print(data_dihapus.head())\n",
    "\n",
    "    df.to_csv(\"nonfictionn_clean.csv\", index=False, sep=',')\n",
    "\n",
    "    print(\"Data yang telah dibersihkan disimpan ke 'nonfictionn_clean.csv'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6k6f0P0oQvRR"
   },
   "source": [
    "**DATA CLEANING DATASET GENRE RELIGI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4yzGKO2Emcub",
    "outputId": "7963121c-8481-43c8-9481-c4a10f6cb4b6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def bersihkan_teks(teks):\n",
    "    teks_bersih = re.sub(r'[^a-zA-Z0-9\\s.,!?()-]', '', str(teks))\n",
    "    teks_bersih = ' '.join(teks_bersih.split())\n",
    "    return teks_bersih\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        \"/content/religi_clean.csv\",\n",
    "        sep=';',\n",
    "        quotechar='\"',\n",
    "        escapechar='\\\\',\n",
    "        on_bad_lines='skip',\n",
    "        engine='python'\n",
    "    )\n",
    "\n",
    "    if len(df.columns) == 1:\n",
    "        df = pd.read_csv(\n",
    "            \"/content/religi_clean.csv\",\n",
    "            sep=',',\n",
    "            quotechar='\"',\n",
    "            escapechar='\\\\',\n",
    "            on_bad_lines='skip',\n",
    "            engine='python',\n",
    "            header=None\n",
    "        )\n",
    "        df.columns = ['Title', 'Author', 'Link', 'Sinopsis', 'Genre']\n",
    "\n",
    "    df_awal = df.copy()\n",
    "    jumlah_awal = len(df)\n",
    "\n",
    "    df = df[df['Sinopsis'].notna()]\n",
    "    df = df[df['Sinopsis'].str.strip() != '']\n",
    "    df = df[df['Genre'] != 'Genre not found']\n",
    "    df['Sinopsis'] = df['Sinopsis'].apply(bersihkan_teks)\n",
    "    df = df.drop_duplicates(subset='Sinopsis', keep='first')\n",
    "\n",
    "    data_dihapus = df_awal[~df_awal.index.isin(df.index)]\n",
    "    jumlah_akhir = len(df)\n",
    "    jumlah_dihapus = jumlah_awal - jumlah_akhir\n",
    "\n",
    "    print(f\"Jumlah baris yang dihapus: {jumlah_dihapus}\")\n",
    "    print(f\"Jumlah baris yang tersisa: {jumlah_akhir}\")\n",
    "    print(\"Sampel data yang dihapus:\")\n",
    "    print(data_dihapus.head())\n",
    "\n",
    "    df.to_csv(\"religionn_clean.csv\", index=False, sep=',')\n",
    "\n",
    "    print(\"Data yang telah dibersihkan disimpan ke 'religionn_clean.csv'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-Md_s5_Qykk"
   },
   "source": [
    "**DATA CLEANING (6 GENRE DIJADIKAN 1 DATASET) (Remove NaN, Genre not found, Sinopsis duplikat, Sinopsis not in english)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXa46pzNekXQ",
    "outputId": "6c6ebfb5-46ee-4b6f-a6ce-8d5e6922a618"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def bersihkan_teks(teks):\n",
    "    teks_bersih = re.sub(r'[^a-zA-Z0-9\\s.,!?()-]', '', str(teks))\n",
    "    teks_bersih = ' '.join(teks_bersih.split())\n",
    "    return teks_bersih\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        \"/content/data full power.csv\",\n",
    "        sep=';',\n",
    "        quotechar='\"',\n",
    "        escapechar='\\\\',\n",
    "        on_bad_lines='skip',\n",
    "        engine='python'\n",
    "    )\n",
    "\n",
    "    print(\"Nama kolom dalam dataset:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    if len(df.columns) == 1:\n",
    "        df = pd.read_csv(\n",
    "            \"/content/data full power.csv\",\n",
    "            sep=',',\n",
    "            quotechar='\"',\n",
    "            escapechar='\\\\',\n",
    "            on_bad_lines='skip',\n",
    "            engine='python',\n",
    "            header=None\n",
    "        )\n",
    "        df.columns = ['Title', 'Author', 'Link', 'Sinopsis', 'Genre']\n",
    "\n",
    "    df_awal = df.copy()\n",
    "    data_nan = df[df['Sinopsis'].isna() | df['Sinopsis'].str.strip().eq('')]\n",
    "    df = df.drop(data_nan.index)\n",
    "    data_genre_not_found = df[df['Genre'] == 'Genre not found']\n",
    "    df = df.drop(data_genre_not_found.index)\n",
    "    df['Sinopsis'] = df['Sinopsis'].apply(bersihkan_teks)\n",
    "    data_duplikat = df[df.duplicated(subset='Sinopsis', keep='first')]\n",
    "    df = df.drop(data_duplikat.index)\n",
    "    jumlah_awal = len(df_awal)\n",
    "    jumlah_akhir = len(df)\n",
    "    jumlah_dihapus = jumlah_awal - jumlah_akhir\n",
    "\n",
    "    print(f\"\\nJumlah baris awal: {jumlah_awal}\")\n",
    "    print(f\"Jumlah baris akhir: {jumlah_akhir}\")\n",
    "    print(f\"Jumlah baris yang dihapus: {jumlah_dihapus}\")\n",
    "\n",
    "    print(\"\\nData yang dihapus karena NaN atau kosong:\")\n",
    "    print(data_nan.head())\n",
    "    print(\"\\nData yang dihapus karena genre 'Genre not found':\")\n",
    "    print(data_genre_not_found.head())\n",
    "    print(\"\\nData yang dihapus karena duplikat:\")\n",
    "    print(data_duplikat.head())\n",
    "\n",
    "    if not data_nan.empty:\n",
    "        data_nan.to_csv(\"data_dihapus_nan.csv\", index=False, sep=',')\n",
    "    if not data_genre_not_found.empty:\n",
    "        data_genre_not_found.to_csv(\"data_dihapus_genre_not_found.csv\", index=False, sep=',')\n",
    "    if not data_duplikat.empty:\n",
    "        data_duplikat.to_csv(\"data_dihapus_duplikat.csv\", index=False, sep=',')\n",
    "    df.to_csv(\"DATA FULL.csv\", index=False, sep=',')\n",
    "    print(\"\\nData yang telah dibersihkan disimpan ke 'DATA FULL.csv'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1lvO80-L_vK",
    "outputId": "79f07e68-f1a1-492c-d53e-3cf7e8005af1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('DATA FULL (gabungan 6 genre).csv', encoding='utf-8', engine='python', sep=None, on_bad_lines='skip')\n",
    "except Exception as e:\n",
    "    print(f\"Error membaca file: {e}\")\n",
    "    print(\"Mencoba metode alternatif...\")\n",
    "    df = pd.read_csv('DATA FULL (gabungan 6 genre).csv', encoding='utf-8', engine='python', sep=None)\n",
    "\n",
    "print(\"Kolom yang tersedia:\")\n",
    "print(df.columns)\n",
    "\n",
    "if 'Sinopsis' in df.columns:\n",
    "    sinopsis_column = 'Sinopsis'\n",
    "elif 'sinopsis' in df.columns:\n",
    "    sinopsis_column = 'sinopsis'\n",
    "else:\n",
    "    raise ValueError(\"Kolom Sinopsis tidak ditemukan dalam dataset\")\n",
    "\n",
    "df = df.dropna(subset=[sinopsis_column])\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(str(text)) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "df_english = df[df[sinopsis_column].apply(is_english)]\n",
    "df_non_english = df[~df[sinopsis_column].apply(is_english)]\n",
    "\n",
    "df_english.to_csv('data_clean_english.csv', index=False, encoding='utf-8')\n",
    "\n",
    "df_non_english.to_csv('data_non_english.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Jumlah data awal: {len(df)}\")\n",
    "print(f\"Jumlah data berbahasa Inggris: {len(df_english)}\")\n",
    "print(f\"Jumlah data yang dihapus (bukan bahasa Inggris): {len(df_non_english)}\")\n",
    "\n",
    "print(\"\\nContoh data yang dihapus:\")\n",
    "print(df_non_english.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwfGrST6RNnf"
   },
   "source": [
    "**LOWERCASING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4GLEnwSHMAcS",
    "outputId": "86d6a07f-618f-4cb3-f374-d6cdda6af5ca"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('data_clean_english.csv', encoding='utf-8')\n",
    "except Exception as e:\n",
    "    print(f\"Error membaca file: {e}\")\n",
    "    print(\"Mencoba metode alternatif...\")\n",
    "    df = pd.read_csv('data_clean_english.csv', encoding='utf-8', engine='python', sep=None)\n",
    "\n",
    "print(\"Kolom yang tersedia:\")\n",
    "print(df.columns)\n",
    "\n",
    "sinopsis_column = 'Sinopsis' if 'Sinopsis' in df.columns else 'sinopsis'\n",
    "\n",
    "if sinopsis_column not in df.columns:\n",
    "    raise ValueError(\"Kolom Sinopsis tidak ditemukan dalam dataset\")\n",
    "\n",
    "df = df.dropna(subset=[sinopsis_column])\n",
    "df[sinopsis_column] = df[sinopsis_column].str.lower()\n",
    "df.to_csv('data_clean_english_lowercase.csv', index=False, encoding='utf-8')\n",
    "print(f\"Jumlah data awal: {len(df)}\")\n",
    "print(f\"Data berhasil disimpan ke 'data_clean_english_lowercase.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-TRT0nY5MAgU",
    "outputId": "36479b57-10a4-4bed-88c8-429abca6aec8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"data_clean_english_lowercase.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RLNdRH7RTHp"
   },
   "source": [
    "**REMOVE PUNCTUATION EXCEPT (,) TITIK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhVO9P7OMAlQ",
    "outputId": "efad4f75-5ee8-4583-ec2b-ccae1c84232c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('data_clean_english_lowercase.csv', encoding='utf-8')\n",
    "\n",
    "if 'Sinopsis' not in df.columns:\n",
    "    raise ValueError(\"Kolom 'Sinopsis' tidak ditemukan dalam dataset!\")\n",
    "\n",
    "def bersihkan_teks(teks):\n",
    "    teks = re.sub(r'[^\\w\\s.]', '', teks)\n",
    "    teks = re.sub(r'\\.{2,}', '.', teks)\n",
    "    teks = teks.strip()\n",
    "    return teks\n",
    "\n",
    "df['Sinopsis_Bersih'] = df['Sinopsis'].apply(bersihkan_teks)\n",
    "df.to_csv('data_cleaned.csv', index=False, encoding='utf-8')\n",
    "print(df[['Sinopsis', 'Sinopsis_Bersih']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McCVES1aRYeT"
   },
   "source": [
    "**TOKENISASI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_Jm5nR2MAqe",
    "outputId": "6a5cc80d-b69f-43d7-f680-7ab9ea409907"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(teks):\n",
    "    return word_tokenize(teks)\n",
    "\n",
    "df['Tokenisasi'] = df['Sinopsis_Bersih'].apply(tokenize)\n",
    "df.to_csv('data_with_tokenization.csv', index=False, encoding='utf-8')\n",
    "print(df[['Sinopsis', 'Tokenisasi']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3A9N0ZWRbRr"
   },
   "source": [
    "**STOPWORDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NR5YPLNgNnxM",
    "outputId": "03b94f9b-f02c-43cd-c912-cdb1e7fb9de3"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def hapus_stopwords(token_list):\n",
    "    return [token for token in token_list if token.lower() not in stop_words]\n",
    "\n",
    "df['Tanpa_Stopwords'] = df['Tokenisasi'].apply(hapus_stopwords)\n",
    "df.to_csv('data_without_stopwords.csv', index=False, encoding='utf-8')\n",
    "print(df[['Sinopsis', 'Tanpa_Stopwords']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPgkp28_Re1D"
   },
   "source": [
    "**LEMATISASI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8vABXxrNn6z",
    "outputId": "1e2449fe-65e1-4699-9c6a-14913b6ce0b6"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizeText(token_list):\n",
    "    return [lemmatizer.lemmatize(token) for token in token_list]\n",
    "\n",
    "df['Lematisasi'] = df['Tanpa_Stopwords'].apply(lemmatizeText)\n",
    "df.to_csv('data_with_lemmatization.csv', index=False, encoding='utf-8')\n",
    "print(df[['Sinopsis', 'Lematisasi']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsSa8SeERjtF"
   },
   "source": [
    "**FEATURE EXTRACTION MENGGUNAKAN TF IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install keras\n",
    "%pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install keras.preprocessing.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsrOrWWnNoGU"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data_with_lemmatization.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkJvmPDlMAvL"
   },
   "outputs": [],
   "source": [
    "bow = CountVectorizer(\n",
    "    max_features=5000, \n",
    "    ngram_range=(1, 1)\n",
    ")\n",
    "\n",
    "X = df['Lematisasi']\n",
    "y = df['Genre']\n",
    "\n",
    "X_bow = bow.fit_transform(X).toarray()\n",
    "\n",
    "pd.DataFrame(X_bow, columns=bow.get_feature_names_out()).to_csv('data_with_bow.csv', index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Mr7_-umRpXn"
   },
   "source": [
    "**TRAINING AND VALIDATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HinMIRVMA0Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_val)\n",
    "\n",
    "print(f'Akurasi: {accuracy_score(y_val, y_pred):.2f}')\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "7QwjbkdOPLJs",
    "outputId": "4e4bba79-f764-4785-fdb3-567f377a88c2"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0Zfw7xhS7x3"
   },
   "source": [
    "**AKURASI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6wCsGzl6PLXY",
    "outputId": "aab0920d-2bcc-4b61-bbfb-441f004c40f9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_pred = svm_model.predict(X_val)\n",
    "\n",
    "print(f'Akurasi: {accuracy_score(y_val, y_pred):.2f}')\n",
    "\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAoP8XmrS_Go"
   },
   "source": [
    "**CONFUSSION MATRIX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "ds6hR75tPLgP",
    "outputId": "1964c5fd-815c-42da-d732-7b852ca9ba30"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "class_labels = sorted(set(y_train) | set(y_val))\n",
    "if len(class_labels) != len(svm_model.classes_):\n",
    "    class_labels = svm_model.classes_\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels\n",
    ")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2MWs4vzPLlk"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Simpan model SVM dan vectorizer\n",
    "with open('svm_model(RBF).pkl', 'wb') as model_file:\n",
    "    pickle.dump(svm_model, model_file)\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "    pickle.dump(tfidf, vectorizer_file)\n",
    "\n",
    "print(\"Model dan vectorizer berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pickle\n",
    "\n",
    "# Muat model dan vectorizer\n",
    "with open('svm_model(RBF).pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as vectorizer_file:\n",
    "    loaded_vectorizer = pickle.load(vectorizer_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprosesing import Preprocessor\n",
    "\n",
    "# Inisialisasi preprocessor\n",
    "preprocessor = Preprocessor(sinopsis_column='Sinopsis')\n",
    "\n",
    "# Data baru\n",
    "data_baru = pd.DataFrame({'Sinopsis': [\"\"\"No religion in the modern world is as feared and misunderstood as Islam. It haunts the popular Western imagination as an extreme faith that promotes authoritarian government, female oppression, civil war, and terrorism. Karen Armstrong's short history offers a vital corrective to this narrow view. The distillation of years of thinking and writing about Islam, it demonstrates that the world's fastest-growing faith is a much richer and more complex phenomenon than its modern fundamentalist strain might suggest.\n",
    "\n",
    "Islam: A Short History begins with the flight of Muhammad and his family from Medina in the seventh century and the subsequent founding of the first mosques. It recounts the origins of the split between Shii and Sunni Muslims, and the emergence of Sufi mysticism; the spread of Islam throughout North Africa, the Levant, and Asia; the shattering effect on the Muslim world of the Crusades; the flowering of imperial Islam in the fourteenth and fifteenth centuries into the world's greatest and most sophisticated power; and the origins and impact of revolutionary Islam. It concludes with an assessment of Islam today and its challenges.\n",
    "\n",
    "With this brilliant book, Karen Armstrong issues a forceful challenge to those who hold the view that the West and Islam are civilizations set on a collision course. It is also a model of authority, elegance, and economy.\"\"\"]})\n",
    "\n",
    "# Preprocessing data baru\n",
    "data_baru_processed = preprocessor.preprocess_dataframe(data_baru)\n",
    "\n",
    "# Prediksi dengan model\n",
    "sinopsis_tfidf = loaded_vectorizer.transform(data_baru_processed['Lemmatized'].apply(' '.join))\n",
    "sinopsis_dense = sinopsis_tfidf.toarray()  # Ubah sparse matrix menjadi dense array\n",
    "prediksi = loaded_model.predict(sinopsis_dense)\n",
    "\n",
    "print(f\"Genre yang diprediksi: {prediksi[0]}\")\n",
    "data_baru_processed['Lemmatized']\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
